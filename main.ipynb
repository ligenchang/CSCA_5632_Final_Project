{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8148ac3f",
   "metadata": {},
   "source": [
    "# Assignment: CSCA 5632 Unsupervised Algorithms for Customer Segmentation and Recommendation Systems\n",
    "\n",
    "## Introduction\n",
    "\n",
    "E-commerce businesses generate vast amounts of transaction data, but extracting meaningful insights from this data remains challenging. Key business goals include understanding customer behavior, providing personalized product recommendations, and predicting future purchases. Addressing these challenges can improve marketing effectiveness, increase customer retention, and boost sales.\n",
    "\n",
    "In this assignment, I focus on applying unsupervised machine learning techniques to analyze real-world e-commerce data. The main objectives are:\n",
    "\n",
    "1. Customer segmentation to distinct groups of customers based on their purchasing behavior.  \n",
    "2. Product recommendation to suggest relevant items to customers to enhance cross-selling and upselling opportunities.\n",
    "\n",
    "By exploring these patterns, I aim to demonstrate how data-driven insights can inform better business decisions in online retail.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "- Dataset: UCI Online Retail Dataset  \n",
    "- Source: https://archive.ics.uci.edu/dataset/352/online+retail  \n",
    "- Business Context: Transactions from a UK-based online retailer specializing in gifts, including both individual and wholesale customers.  \n",
    "- Timeframe: December 2010 – December 2011  \n",
    "- Size: 541,909 transactions, 8 attributes\n",
    "\n",
    "This dataset provides a realistic snapshot of e-commerce operations, capturing diverse purchasing behaviors across a range of products.\n",
    "\n",
    "## Problem Definition\n",
    "\n",
    "E-commerce businesses collect large volumes of transaction data, but turning this data into actionable insights is not straightforward. Two key challenges are particularly important: understanding different customer groups and making personalized product recommendations.\n",
    "\n",
    "Without effective segmentation, marketing campaigns tend to be generic, leading to low engagement and wasted advertising budgets. Similarly, poor recommendation systems can result in missed cross-selling opportunities, fewer repeat purchases, and higher customer churn.\n",
    "\n",
    "In this project, I aim to use unsupervised learning methods, such as clustering based on purchase history and customer behavior, to identify meaningful customer segments. I expect that understanding these segments will help deliver better recommendations, improve customer lifetime value, and increase sales.\n",
    "\n",
    "## Why I Chose This Dataset\n",
    "\n",
    "I chose this dataset because it represents real-world e-commerce data, rather than a simplified or synthetic dataset. This means it includes imperfections, inconsistencies, and missing values, which provide an opportunity to practice data cleaning, transformation, and exploratory analysis in a realistic context.\n",
    "\n",
    "Its richness and complexity make it well-suited for unsupervised learning techniques such as clustering, dimensionality reduction, and matrix factorization. These methods can uncover hidden patterns in customer behavior, purchasing preferences, and product relationships. Working with this dataset allows me to extract practical insights that closely mirror real business scenarios and challenges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf42b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cfc060",
   "metadata": {},
   "source": [
    "# Online Retail Data Loading\n",
    "\n",
    "It loads the **Online Retail** dataset from an Excel file into a Pandas DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e845b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = 'online_retail.xlsx'\n",
    "\n",
    "df = pd.read_excel(data_path)\n",
    "display(df.head())\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37603dc",
   "metadata": {},
   "source": [
    "# Dataset Overview\n",
    "\n",
    "This shows a quick summary of the dataset using Pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4fd79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['InvoiceDate'].min()} to {df['InvoiceDate'].max()}\")\n",
    "print(f\"Unique customers: {df['Customer ID'].nunique()}\")\n",
    "print(f\"Unique products: {df['Description'].nunique()}\")\n",
    "print(f\"Unique countries: {df['Country'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f182676a",
   "metadata": {},
   "source": [
    "# Missing Data Analysis\n",
    "\n",
    "This code computes the missing values in each column of the dataset and shows missing counts and percentages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3fb055",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Percentage': missing_percent\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "print(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e584b49",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis: Online Retail Dataset\n",
    "\n",
    "This exploratory data analysis gives an overview of customer behavior, product popularity, and sales trends in the Online Retail dataset. Most purchases involve moderate quantities and affordable unit prices, while sales are concentrated in a few key countries. Clear monthly and weekday patterns emerge, with distinct peak hours during the day. A small number of products drive most of the purchases, and revenue distribution is uneven, showing the presence of high-value customers. Overall, customer engagement varies widely, with some buyers being highly active and others purchasing only occasionally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0edc708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "fig, axes = plt.subplots(3, 3, figsize=(24, 14))  # wider and shorter\n",
    "axes = axes.flatten()\n",
    "\n",
    "# ---  Quantity distribution ---\n",
    "quantity_filtered = df[df['Quantity'].between(0, 100)]['Quantity']\n",
    "axes[0].hist(quantity_filtered, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Quantity Distribution (0–100)', fontsize=14)\n",
    "axes[0].set_xlabel('Quantity')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# ---  Price distribution ---\n",
    "price_filtered = df[df['Price'].between(0, 50)]['Price']\n",
    "axes[1].hist(price_filtered, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title('Price Distribution (0–50)', fontsize=14)\n",
    "axes[1].set_xlabel('Price (£)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "# ---  Top countries ---\n",
    "top_countries = df['Country'].value_counts().head(10)\n",
    "top_countries.plot(kind='bar', ax=axes[2], color=\"skyblue\", edgecolor=\"black\")\n",
    "axes[2].set_title('Top 10 Countries by Transactions', fontsize=14)\n",
    "axes[2].set_xlabel('Country')\n",
    "axes[2].set_ylabel('Transaction Count')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# ---  Monthly sales trend ---\n",
    "df['YearMonth'] = pd.to_datetime(df['InvoiceDate']).dt.to_period('M')\n",
    "monthly_sales = df.groupby('YearMonth')['Price'].sum()\n",
    "monthly_sales.plot(kind='line', marker='o', ax=axes[3], color=\"darkblue\")\n",
    "axes[3].set_title('Monthly Sales Trend', fontsize=14)\n",
    "axes[3].set_xlabel('Month')\n",
    "axes[3].set_ylabel('Total Sales (£)')\n",
    "axes[3].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# ---  Customer transaction frequency ---\n",
    "customer_transactions = df.groupby('Customer ID')['Invoice'].nunique()\n",
    "axes[4].hist(customer_transactions, bins=50, edgecolor='black', alpha=0.7, color=\"orange\")\n",
    "axes[4].set_title('Customer Transaction Frequency', fontsize=14)\n",
    "axes[4].set_xlabel('Transactions per Customer')\n",
    "axes[4].set_ylabel('Number of Customers')\n",
    "\n",
    "# ---  Top products ---\n",
    "top_products = df['Description'].value_counts().head(15)\n",
    "top_products.plot(kind='barh', ax=axes[5], color=\"teal\")\n",
    "axes[5].set_title('Top 15 Most Popular Products', fontsize=14)\n",
    "axes[5].set_xlabel('Purchase Count')\n",
    "axes[5].set_ylabel('Product')\n",
    "axes[5].tick_params(axis='y', labelsize=9)  # smaller font for long names\n",
    "\n",
    "# ---  Revenue per customer ---\n",
    "customer_revenue = df.groupby('Customer ID')['Price'].sum()\n",
    "axes[6].hist(customer_revenue[customer_revenue <= 1000], bins=50, edgecolor='black', alpha=0.7, color=\"purple\")\n",
    "axes[6].set_title('Customer Revenue Distribution (≤£1000)', fontsize=14)\n",
    "axes[6].set_xlabel('Revenue (£)')\n",
    "axes[6].set_ylabel('Customers')\n",
    "\n",
    "# ---  Weekday analysis ---\n",
    "df['Weekday'] = pd.to_datetime(df['InvoiceDate']).dt.day_name()\n",
    "weekday_sales = df.groupby('Weekday')['Price'].sum()\n",
    "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "weekday_sales = weekday_sales.reindex(weekday_order)\n",
    "weekday_sales.plot(kind='bar', ax=axes[7], color=\"salmon\", edgecolor=\"black\")\n",
    "axes[7].set_title('Sales by Weekday', fontsize=14)\n",
    "axes[7].set_xlabel('Day of Week')\n",
    "axes[7].set_ylabel('Total Sales (£)')\n",
    "axes[7].tick_params(axis='x', rotation=30)\n",
    "\n",
    "# ---  Hourly transactions ---\n",
    "df['Hour'] = pd.to_datetime(df['InvoiceDate']).dt.hour\n",
    "hourly_transactions = df.groupby('Hour').size()\n",
    "hourly_transactions.plot(kind='line', marker='o', ax=axes[8], color=\"green\")\n",
    "axes[8].set_title('Transactions by Hour', fontsize=14)\n",
    "axes[8].set_xlabel('Hour of Day')\n",
    "axes[8].set_ylabel('Transactions')\n",
    "\n",
    "plt.subplots_adjust(\n",
    "    top=0.95, \n",
    "    bottom=0.05, \n",
    "    left=0.05, \n",
    "    right=0.98, \n",
    "    hspace=0.35, \n",
    "    wspace=0.25\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60b441a",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "The dataset required thorough cleaning to ensure reliable analysis. This involved removing cancelled orders, invalid quantities and prices, missing or duplicate customer records, and incorrect dates. All columns were standardized, and the data was reindexed to create a consistent and accurate dataset for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0096a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Remove cancelled invoices (InvoiceNo starting with 'C')\n",
    "df_clean = df_clean[~df_clean['Invoice'].astype(str).str.startswith('C')]\n",
    "\n",
    "# Remove negative or zero Quantity and Price\n",
    "df_clean = df_clean[(df_clean['Quantity'] > 0) & (df_clean['Price'] > 0)]\n",
    "\n",
    "# Drop rows with missing CustomerID\n",
    "df_clean = df_clean.dropna(subset=['Customer ID'])\n",
    "\n",
    "# Drop duplicates\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "\n",
    "# Ensure InvoiceDate is datetime and handle errors\n",
    "df_clean['InvoiceDate'] = pd.to_datetime(df_clean['InvoiceDate'], errors='coerce')\n",
    "df_clean = df_clean.dropna(subset=['InvoiceDate'])  # Remove rows where date conversion failed\n",
    "df_clean['Customer ID'] = df_clean['Customer ID'].astype('int64')\n",
    "\n",
    "\n",
    "df_clean = df_clean.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed37624",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original shape:\", df.shape)\n",
    "print(\"Cleaned shape :\", df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd3f633",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Missing Values After Cleaning ---\")\n",
    "print(df_clean.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483988b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Sample Clean Data ---\")\n",
    "display(df_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83c5f37",
   "metadata": {},
   "source": [
    "## Data Cleaning Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51562d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data cleaning analysis:\")\n",
    "print(f\"Original dataset: {df.shape[0]:,} rows\")\n",
    "print(f\"After cleaning: {df_clean.shape[0]:,} rows\")\n",
    "print(f\"Removed: {df.shape[0] - df_clean.shape[0]:,} rows ({(df.shape[0] - df_clean.shape[0])/df.shape[0]*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nQuality checks after cleaning:\")\n",
    "print(f\"- Missing Customer IDs: {df_clean['Customer ID'].isnull().sum()}\")\n",
    "print(f\"- Negative quantities: {(df_clean['Quantity'] < 0).sum()}\")\n",
    "print(f\"- Zero/negative prices: {(df_clean['Price'] <= 0).sum()}\")\n",
    "print(f\"- Duplicate rows: {df_clean.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225b3813",
   "metadata": {},
   "source": [
    "## Cleaned Data Statistics\n",
    "\n",
    "The cleaned dataset includes 4,312 unique customers and 4,444 unique products, spanning the period from December 2009 to December 2010. Total revenue amounts to £1.33 million, with an average transaction value of £3.31."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6e64ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- CLEANED DATA CHARACTERISTICS ---\")\n",
    "print(f\"Unique customers: {df_clean['Customer ID'].nunique():,}\")\n",
    "print(f\"Unique products: {df_clean['Description'].nunique():,}\")\n",
    "print(f\"Date range: {df_clean['InvoiceDate'].min()} to {df_clean['InvoiceDate'].max()}\")\n",
    "print(f\"Total revenue: {df_clean['Price'].sum():,.2f}\")\n",
    "print(f\"Average transaction value: {df_clean['Price'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdefff39",
   "metadata": {},
   "source": [
    "## Customer Metrics Overview\n",
    "\n",
    "After cleaning and removing outliers, the dataset includes 4,268 customers. The following visualization highlights the distribution of recency, frequency, and monetary metrics, with RFM for short. Skewed metrics were log-transformed to normalize their distributions, which is important for clustering methods that rely on distance measures. Recency was not log-transformed, as it does not exhibit a long-tailed distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd4c56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "ref_date = df_clean['InvoiceDate'].max() + pd.Timedelta(days=1)\n",
    "print(f\"Reference date for recency: {ref_date}\")\n",
    "\n",
    "rfm = df_clean.groupby('Customer ID').agg({\n",
    "    'InvoiceDate': lambda x: (ref_date - x.max()).days,  # Recency (days)\n",
    "    'Invoice': 'nunique',                                # Frequency (unique invoices)\n",
    "    'Price': 'sum'                                       # Monetary (total spent)\n",
    "}).reset_index()\n",
    "\n",
    "rfm.columns = ['CustomerID', 'Recency', 'Frequency', 'Monetary']\n",
    "\n",
    "customer_metrics = df_clean.groupby('Customer ID').agg({\n",
    "    'Quantity': ['sum', 'mean'],                         # Total and average quantity\n",
    "    'Description': 'nunique',                            # Product variety\n",
    "    'InvoiceDate': lambda x: (x.max() - x.min()).days   # Customer lifetime span\n",
    "}).reset_index()\n",
    "\n",
    "customer_metrics.columns = ['CustomerID', 'TotalQuantity', 'AvgQuantity', 'ProductVariety', 'LifetimeSpan']\n",
    "rfm = rfm.merge(customer_metrics, on='CustomerID')\n",
    "\n",
    "print(f\"Before outlier removal: {len(rfm)} customers\")\n",
    "rfm = rfm[(rfm['Monetary'] > 0) & (rfm['Frequency'] > 0)]\n",
    "rfm = rfm[rfm['Monetary'] <= rfm['Monetary'].quantile(0.99)]  # Remove top 1% to reduce skewness\n",
    "print(f\"After outlier removal: {len(rfm)} customers\")\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "for i, col in enumerate(['Recency', 'Frequency', 'Monetary', 'TotalQuantity', 'ProductVariety']):\n",
    "    plt.subplot(3, 5, i+1)\n",
    "    sns.histplot(rfm[col], kde=True, bins=30)\n",
    "    plt.title(f'{col} Distribution')\n",
    "    \n",
    "    # Add statistical annotations\n",
    "    mean_val = rfm[col].mean()\n",
    "    median_val = rfm[col].median()\n",
    "    plt.axvline(mean_val, color='red', linestyle='--', label=f'Mean: {mean_val:.1f}')\n",
    "    plt.axvline(median_val, color='green', linestyle='--', label=f'Median: {median_val:.1f}')\n",
    "    plt.legend(fontsize=8)\n",
    "\n",
    "rfm_log = rfm.copy()\n",
    "for col in ['Frequency', 'Monetary', 'TotalQuantity', 'ProductVariety']:\n",
    "    rfm_log[col] = np.log1p(rfm_log[col])\n",
    "\n",
    "# Log-transformed distributions\n",
    "for i, col in enumerate(['Recency', 'Frequency', 'Monetary', 'TotalQuantity', 'ProductVariety']):\n",
    "    plt.subplot(3, 5, i+6)\n",
    "    sns.histplot(rfm_log[col], kde=True, bins=30)\n",
    "    plt.title(f'{col} (Log-transformed)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(rfm[['Recency', 'Frequency', 'Monetary', 'TotalQuantity', 'ProductVariety']].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257a74dd",
   "metadata": {},
   "source": [
    "## RFM Feature Correlation Analysis\n",
    "\n",
    "The correlation analysis of RFM features reveals several notable patterns in customer behavior. There is a strong positive correlation between monetary value and product variety (r = 0.88), indicating that customers who spend more also tend to purchase a wider range of products. Frequency and monetary value are moderately correlated (r = 0.66), suggesting that customers who make purchases more often also tend to spend more overall. Additionally, frequency and product variety show a positive correlation (r = 0.55), meaning that frequent buyers are more likely to explore and purchase from a broader selection of products. These insights highlight the relationships between how often customers buy, how much they spend, and the diversity of their purchases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2637d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "correlation_matrix = rfm[['Recency', 'Frequency', 'Monetary', 'TotalQuantity', 'ProductVariety']].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    correlation_matrix, \n",
    "    annot=True, \n",
    "    cmap='coolwarm', \n",
    "    center=0, \n",
    "    fmt='.2f',\n",
    "    linewidths=0.5,\n",
    "    cbar=True\n",
    ")\n",
    "plt.title('Feature Correlations', fontsize=14, pad=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020efb27",
   "metadata": {},
   "source": [
    "## PC Analysis\n",
    "Reducing the RFM features to 2 dimensions using PCA serves a few important purposes. First, it simplifies the data while retaining most of the original variability, making it easier to visualize and interpret customer patterns. Working in a lower-dimensional space also removes correlations between features, which helps clustering algorithms like K-Means perform more effectively because they rely on distance measures. Finally, reducing to two dimensions allows us to plot the customers on a 2D scatter, providing an intuitive view of the clusters and helping validate the quality of the segmentation visually. Overall, PCA makes the clustering process faster, more robust, and easier to interpret without losing key information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b8d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "feature_cols = ['Recency', 'Frequency', 'Monetary']\n",
    "X = rfm_log[feature_cols].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "rfm['PCA1'] = X_pca[:, 0]\n",
    "rfm['PCA2'] = X_pca[:, 1]\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained variance by each component: {explained_variance}\")\n",
    "print(f\"Total variance explained by 2 components: {explained_variance.sum():.2f}\")\n",
    "\n",
    "pca_components = pd.DataFrame(\n",
    "    pca.components_,\n",
    "    columns=feature_cols,\n",
    "    index=['PCA1', 'PCA2']\n",
    ")\n",
    "pca_components = pca_components.round(3) \n",
    "\n",
    "print(\"\\nPCA Composition Table:\")\n",
    "display(pca_components)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6, edgecolor='k')\n",
    "plt.xlabel(f'PCA1 ({explained_variance[0]*100:.1f}% variance)')\n",
    "plt.ylabel(f'PCA2 ({explained_variance[1]*100:.1f}% variance)')\n",
    "plt.title('2D PCA of RFM Features')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef059b8-4d34-4816-b9de-ddcb9ba436f4",
   "metadata": {},
   "source": [
    "# K-Means Clustering for Customers segmentation\n",
    "\n",
    "K-Means clustering was applied to the scaled RFM features, with the number of clusters (k) varied from 2 to 10 in a parameter sweep. The clustering results were evaluated using Inertia, Silhouette Score, and Davies-Bouldin Index to assess cluster compactness and separation. Based on these metrics, 3 clusters were identified as the optimal choice, providing a good balance between cohesion within clusters and separation between clusters. Visualizations of the clusters further confirm the quality of this segmentation and support the selection of the optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e76b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- K-Means clustering hyperparameter sweeping---\n",
    "inertia, silhouette, davies_bouldin = [], [], []\n",
    "K = range(2, 11)\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=20, max_iter=300)\n",
    "    labels = kmeans.fit_predict(X_pca)\n",
    "    \n",
    "    inertia.append(kmeans.inertia_)\n",
    "    silhouette.append(silhouette_score(X_pca, labels))\n",
    "    davies_bouldin.append(davies_bouldin_score(X_pca, labels))\n",
    "    \n",
    "    print(f\"k={k}: Inertia={kmeans.inertia_:.2f}, \"\n",
    "          f\"Silhouette={silhouette_score(X_pca, labels):.3f}, \"\n",
    "          f\"Davies-Bouldin={davies_bouldin_score(X_pca, labels):.3f}\")\n",
    "\n",
    "norm_inertia = [(max(inertia)-i)/(max(inertia)-min(inertia)) for i in inertia]\n",
    "norm_silhouette = [(s - min(silhouette))/(max(silhouette)-min(silhouette)) for s in silhouette]\n",
    "norm_db = [(max(davies_bouldin)-db)/(max(davies_bouldin)-min(davies_bouldin)) for db in davies_bouldin]\n",
    "combined_score = [sum(x) for x in zip(norm_inertia, norm_silhouette, norm_db)]\n",
    "optimal_k = K[combined_score.index(max(combined_score))]\n",
    "print(f\"\\nOptimal number of clusters: {optimal_k}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "axes[0, 0].plot(K, inertia, 'bo-', markersize=8, linewidth=2)\n",
    "axes[0, 0].set_title(\"Elbow Method\")\n",
    "axes[0, 0].set_xlabel(\"k\")\n",
    "axes[0, 0].set_ylabel(\"Inertia\")\n",
    "\n",
    "axes[0, 1].plot(K, silhouette, 'ro-', markersize=8, linewidth=2)\n",
    "axes[0, 1].set_title(\"Silhouette Score\")\n",
    "axes[0, 1].set_xlabel(\"k\")\n",
    "axes[0, 1].set_ylabel(\"Score\")\n",
    "\n",
    "axes[1, 0].plot(K, davies_bouldin, 'go-', markersize=8, linewidth=2)\n",
    "axes[1, 0].set_title(\"Davies-Bouldin Index (lower better)\")\n",
    "axes[1, 0].set_xlabel(\"k\")\n",
    "axes[1, 0].set_ylabel(\"DB Index\")\n",
    "\n",
    "axes[1, 1].plot(K, combined_score, 'mo-', markersize=8, linewidth=2)\n",
    "axes[1, 1].axvline(optimal_k, color='red', linestyle='--', label=f'Optimal k={optimal_k}')\n",
    "axes[1, 1].set_title(\"Combined Score\")\n",
    "axes[1, 1].set_xlabel(\"k\")\n",
    "axes[1, 1].set_ylabel(\"Normalized Score\")\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eda82f4-ebca-46b8-8863-c369ceaa8b0f",
   "metadata": {},
   "source": [
    "# Final K-Means Clustering (k=3)\n",
    "\n",
    "The final K-Means clustering model (k=3) produced a silhouette score of 0.390, indicating moderate cluster separation. The clusters are distributed fairly evenly.\n",
    "\n",
    "Cluster 0 represents customers with high recency but low frequency and spending, suggesting recent but low-value engagement. Cluster 1 includes highly active and valuable customers, with high frequency, monetary value, and product variety, reflecting loyal and engaged buyers. Cluster 2 captures customers with moderate recency, low to moderate frequency, and spending, representing occasional or moderately engaged customers. These clusters provide insights into customer behavior, helping guide targeted marketing and retention strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ed00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_final = 3\n",
    "kmeans_final = KMeans(n_clusters=k_final, random_state=42, n_init=20, max_iter=300, algorithm='lloyd')\n",
    "rfm['Cluster'] = kmeans_final.fit_predict(X_pca)\n",
    "\n",
    "final_silhouette = silhouette_score(X_scaled, rfm['Cluster'])\n",
    "print(f\"Final K-means model performance:\")\n",
    "print(f\"- Silhouette score: {final_silhouette:.3f}\")\n",
    "\n",
    "cluster_counts = rfm['Cluster'].value_counts().sort_index()\n",
    "print(\"Cluster distribution:\")\n",
    "for cluster, count in cluster_counts.items():\n",
    "    print(f\"Cluster {cluster}: {count:,} customers ({count/len(rfm)*100:.1f}%)\")\n",
    "\n",
    "cluster_profiles = rfm.groupby('Cluster')[['Recency','Frequency','Monetary','TotalQuantity','ProductVariety','LifetimeSpan']].agg(['mean','median','std']).round(2)\n",
    "display(cluster_profiles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0db0ec",
   "metadata": {},
   "source": [
    "# Final K-Means Cluster Visualization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a28b264",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "sns.scatterplot(x='Recency', y='Monetary', hue='Cluster', palette='Set2', data=rfm, ax=axes[0, 0], s=60, alpha=0.7)\n",
    "axes[0, 0].set_title(\"K-Means (k=3): Recency vs Monetary\")\n",
    "axes[0, 0].set_xlabel(\"Recency (days)\")\n",
    "axes[0, 0].set_ylabel(\"Monetary (£)\")\n",
    "\n",
    "sns.scatterplot(x='Frequency', y='Monetary', hue='Cluster', palette='Set2', data=rfm, ax=axes[0, 1], s=60, alpha=0.7)\n",
    "axes[0, 1].set_title(\"K-Means (k=3): Frequency vs Monetary\")\n",
    "axes[0, 1].set_xlabel(\"Frequency (transactions)\")\n",
    "axes[0, 1].set_ylabel(\"Monetary (£)\")\n",
    "\n",
    "rfm_melted = pd.melt(rfm[['Cluster','Recency','Frequency','Monetary']], id_vars=['Cluster'], var_name='RFM_Dimension', value_name='Value')\n",
    "sns.boxplot(data=rfm_melted[rfm_melted['RFM_Dimension']=='Recency'], x='Cluster', y='Value', ax=axes[1, 0], palette='Set2', hue='Cluster')\n",
    "axes[1, 0].set_title(\"K-Means (k=3): Recency by Cluster\")\n",
    "axes[1, 0].set_ylabel(\"Recency (days)\")\n",
    "\n",
    "sns.boxplot(data=rfm_melted[rfm_melted['RFM_Dimension']=='Monetary'], x='Cluster', y='Value', ax=axes[1, 1], palette='Set2', hue='Cluster')\n",
    "axes[1, 1].set_title(\"K-Means (k=3): Monetary by Cluster\")\n",
    "axes[1, 1].set_ylabel(\"Monetary (£)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d876617c",
   "metadata": {},
   "source": [
    "# K-Means Clustering Visualization with centroid (k=3) \n",
    "\n",
    "\n",
    "| Cluster | Recency       | Frequency      | Monetary Value | Product Variety | Description                              |\n",
    "|---------|---------------|----------------|----------------|----------------|------------------------------------------|\n",
    "| 0       | High          | Low            | Low            | Low            | low-value customers           |\n",
    "| 1       | Medium/Low    | High           | High           | High           | Loyal and highly engaged customers       |\n",
    "| 2       | Moderate      | Low to Moderate| Low to Moderate| Moderate       | moderately engaged customers|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d4fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "centroids_pca = kmeans_final.cluster_centers_\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    data=rfm, x='PCA1', y='PCA2',\n",
    "    hue='Cluster', palette='Set2',\n",
    "    s=60, alpha=0.6\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    centroids_pca[:, 0], centroids_pca[:, 1],\n",
    "    c='black', marker='X', s=250, label='Centroids'\n",
    ")\n",
    "\n",
    "for i, (x, y) in enumerate(centroids_pca):\n",
    "    plt.text(x + 0.1, y + 0.1, f'Cluster {i}', fontsize=12, weight='bold')\n",
    "\n",
    "plt.title(f'K-Means Clusters in 2D PCA Space (k={optimal_k})', fontsize=16)\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7110984",
   "metadata": {},
   "source": [
    "\n",
    "# Agglomerative Clustering for customer segmentation\n",
    "\n",
    "Agglomerative clustering was applied to PCA-reduced RFM features (2 components) for 4,268 customers, testing 3–7 clusters with ward, complete, average, and single linkages. The best result was 3 clusters with Ward linkage, achieving a silhouette score of around 0.41. Ward linkage produced compact, well-separated clusters, and the silhouette decline with more clusters indicates potential over-segmentation. Selecting 3 clusters aligns with standard RFM segmentation (High, Medium, Low value), balancing statistical separation and business interpretability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4a5408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "n_clusters_range = range(3, 8)\n",
    "linkage_methods = ['ward', 'complete', 'average', 'single']\n",
    "\n",
    "results = []\n",
    "\n",
    "for n_clusters in n_clusters_range:\n",
    "    for linkage_method in linkage_methods:\n",
    "        metric = 'euclidean' if linkage_method == 'ward' else 'manhattan'\n",
    "        \n",
    "        agglo = AgglomerativeClustering(\n",
    "            n_clusters=n_clusters,\n",
    "            linkage=linkage_method,\n",
    "            metric=metric\n",
    "        )\n",
    "        labels = agglo.fit_predict(X_pca)\n",
    "        \n",
    "        sil = silhouette_score(X_pca, labels)\n",
    "        results.append({\n",
    "            'n_clusters': n_clusters,\n",
    "            'linkage': linkage_method,\n",
    "            'silhouette': sil\n",
    "        })\n",
    "        print(f\"Clusters={n_clusters}, Linkage={linkage_method} -> Silhouette={sil:.3f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "best_idx = results_df['silhouette'].idxmax()\n",
    "best_config = results_df.loc[best_idx]\n",
    "print(\"\\nBest Agglomerative Clustering configuration:\")\n",
    "print(best_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f32ffa4",
   "metadata": {},
   "source": [
    "# Agglomerative Clustering visualziation\n",
    "\n",
    "Agglomerative clustering on PCA-reduced RFM features produced 3 clusters using Ward linkage: Cluster 0 with 2,086 customers (48.9%), Cluster 1 with 1,097 (25.7%), and Cluster 2 with 1,085 (25.4%), achieving a silhouette score of 0.41. Ward linkage generated compact, well-separated clusters, with reasonably balanced sizes that align with typical RFM segmentation (High, Medium, Low value). The silhouette score slightly outperformed K-Means (0.39), highlighting differences between hierarchical and centroid-based approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014f3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, normalized_mutual_info_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "n_clusters = 3\n",
    "linkage_method = \"ward\"\n",
    "\n",
    "agglo = AgglomerativeClustering(\n",
    "    n_clusters=n_clusters, \n",
    "    linkage=linkage_method, \n",
    "    metric='euclidean'\n",
    ")\n",
    "rfm['Agglo_Cluster'] = agglo.fit_predict(X_pca)\n",
    "\n",
    "print(f\"Agglomerative clustering with {n_clusters} clusters completed\")\n",
    "agglo_counts = rfm['Agglo_Cluster'].value_counts().sort_index()\n",
    "for cluster, count in agglo_counts.items():\n",
    "    percentage = count / len(rfm) * 100\n",
    "    print(f\"  Cluster {cluster}: {count:,} customers ({percentage:.1f}%)\")\n",
    "\n",
    "agglo_silhouette = silhouette_score(X_pca, rfm['Agglo_Cluster'])\n",
    "agglo_db = davies_bouldin_score(X_pca, rfm['Agglo_Cluster'])\n",
    "nmi = normalized_mutual_info_score(rfm['Cluster'], rfm['Agglo_Cluster'])\n",
    "\n",
    "print(f\"\\nAgglomerative clustering performance:\")\n",
    "print(f\"- Silhouette score: {agglo_silhouette:.3f}\")\n",
    "\n",
    "\n",
    "# --- Cluster Profile Summary ---\n",
    "cluster_profiles = (\n",
    "    rfm.groupby('Agglo_Cluster')[['Recency', 'Frequency', 'Monetary', 'TotalQuantity', 'ProductVariety', 'LifetimeSpan']]\n",
    "    .agg(['mean', 'median', 'std'])\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "print(\"\\nCluster Profile Summary:\")\n",
    "display(cluster_profiles)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=rfm['Agglo_Cluster'], palette='Set2', s=60, alpha=0.7)\n",
    "plt.title(f\"Agglomerative Clustering ({n_clusters} clusters) on PCA-reduced data\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dce503",
   "metadata": {},
   "source": [
    "# Visualizing Agglomerative Clustering Results\n",
    "\n",
    "The plots show the distribution of Recency, Frequency, and Monetary values across the 3 agglomerative clusters. Scatterplots highlight how clusters separate in key RFM dimensions, while boxplots reveal cluster-specific distributions and central tendencies for Recency and Monetary value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a0a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sample_data = X_scaled if len(rfm) <= 1000 else X_scaled[np.random.choice(len(rfm), 1000, replace=False)]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x='Recency', y='Monetary', hue='Agglo_Cluster', palette='Set2', \n",
    "    data=rfm, ax=axes[0, 0], s=60, alpha=0.7\n",
    ")\n",
    "axes[0, 0].set_title(\"Recency vs Monetary (Agglomerative)\")\n",
    "axes[0, 0].set_xlabel(\"Recency (days)\")\n",
    "axes[0, 0].set_ylabel(\"Monetary (£)\")\n",
    "\n",
    "sns.scatterplot(\n",
    "    x='Frequency', y='Monetary', hue='Agglo_Cluster', palette='Set2', \n",
    "    data=rfm, ax=axes[0, 1], s=60, alpha=0.7\n",
    ")\n",
    "axes[0, 1].set_title(\"Frequency vs Monetary (Agglomerative)\")\n",
    "axes[0, 1].set_xlabel(\"Frequency (transactions)\")\n",
    "axes[0, 1].set_ylabel(\"Monetary (£)\")\n",
    "\n",
    "rfm_melted = pd.melt(\n",
    "    rfm[['Agglo_Cluster','Recency','Frequency','Monetary']], \n",
    "    id_vars=['Agglo_Cluster'], var_name='RFM_Dimension', value_name='Value'\n",
    ")\n",
    "sns.boxplot(\n",
    "    data=rfm_melted[rfm_melted['RFM_Dimension']=='Recency'], \n",
    "    x='Agglo_Cluster', y='Value', ax=axes[1, 0], palette='Set2', hue='Agglo_Cluster'\n",
    ")\n",
    "axes[1, 0].set_title(\"Recency by Agglomerative Cluster\")\n",
    "axes[1, 0].set_ylabel(\"Recency (days)\")\n",
    "\n",
    "sns.boxplot(\n",
    "    data=rfm_melted[rfm_melted['RFM_Dimension']=='Monetary'], \n",
    "    x='Agglo_Cluster', y='Value', ax=axes[1, 1], palette='Set2', hue='Agglo_Cluster'\n",
    ")\n",
    "axes[1, 1].set_title(\"Monetary by Agglomerative Cluster\")\n",
    "axes[1, 1].set_ylabel(\"Monetary (£)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea20f922",
   "metadata": {},
   "source": [
    "# K-Means vs Agglomerative Clustering: Summary & Comparison\n",
    "\n",
    "K-Means and Agglomerative clustering both produced three meaningful customer segments. The silhouette score for Agglomerative clustering (0.410) was slightly higher than for K-Means (0.390), indicating better cluster cohesion. Based on mean RFM values, the clusters can be described as follows:\n",
    "\n",
    "- High-value customers: low Recency, high Frequency and Monetary \n",
    "- Medium-value customers: moderate Recency and Monetary \n",
    "- Low-value customers: high Recency, low Frequency and Monetary\n",
    "\n",
    "K-Means is fast and works well for compact, well-separated clusters because it assumes spherical shapes. Agglomerative clustering does not assume specific cluster shapes and shows hierarchical relationships between customers, giving more insight into nested or irregular patterns. Overall, both methods identify similar customer patterns and can be used for targeted marketing: keeping high-value customers, re-engaging medium-value customers, and trying to revive low-value customers. Agglomerative clustering provides slightly better separation and more structural insight.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074c3104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "\n",
    "print(f\"- K-Means Silhouette: {final_silhouette:.3f}\")\n",
    "print(f\"- Agglomerative Silhouette: {agglo_silhouette:.3f}\")\n",
    "\n",
    "# --- K-Means cluster profiles ---\n",
    "kmeans_profiles = rfm.groupby('Cluster')[['Recency','Frequency','Monetary']].mean().round(2)\n",
    "print(\"K-Means Cluster Profiles (mean RFM values):\")\n",
    "print(kmeans_profiles)\n",
    "\n",
    "# --- Agglomerative cluster profiles ---\n",
    "agglo_profiles = rfm.groupby('Agglo_Cluster')[['Recency','Frequency','Monetary']].mean().round(2)\n",
    "print(\"\\nAgglomerative Cluster Profiles (mean RFM values):\")\n",
    "print(agglo_profiles)\n",
    "fig, axes = plt.subplots(1, 5, figsize=(25, 5))\n",
    "(ax1, ax2, ax3, ax4, ax5) = axes.flatten()\n",
    "\n",
    "# K-Means results\n",
    "ax1.scatter(rfm['PCA1'], rfm['PCA2'], c=rfm['Cluster'], cmap='Set2', alpha=0.7, s=50)\n",
    "ax1.set_title('K-Means Clustering')\n",
    "ax1.set_xlabel('PCA1')\n",
    "ax1.set_ylabel('PCA2')\n",
    "\n",
    "# Agglomerative results\n",
    "ax2.scatter(rfm['PCA1'], rfm['PCA2'], c=rfm['Agglo_Cluster'], cmap='Set2', alpha=0.7, s=50)\n",
    "ax2.set_title(f'Agglomerative Clustering\\n({linkage_method}, k={n_clusters})')\n",
    "ax2.set_xlabel('PCA1')\n",
    "ax2.set_ylabel('PCA2')\n",
    "\n",
    "# Dendrogram\n",
    "sample_data = X_scaled if len(rfm) <= 1000 else X_scaled[np.random.choice(len(rfm), 1000, replace=False)]\n",
    "linkage_matrix = linkage(sample_data, method='ward')\n",
    "dendrogram(linkage_matrix, truncate_mode='level', p=5, ax=ax3)\n",
    "ax3.set_title('Hierarchical Dendrogram')\n",
    "ax3.set_xlabel('Sample Index')\n",
    "ax3.set_ylabel('Distance')\n",
    "\n",
    "# Silhouette comparison\n",
    "methods = ['K-Means', 'Agglomerative']\n",
    "silhouette_scores = [final_silhouette, agglo_silhouette]\n",
    "bars = ax4.bar(methods, silhouette_scores, color=['skyblue', 'lightcoral'])\n",
    "ax4.set_title('Silhouette Score Comparison')\n",
    "ax4.set_ylabel('Silhouette Score')\n",
    "ax4.set_ylim(0, max(silhouette_scores) * 1.1)\n",
    "for bar, score in zip(bars, silhouette_scores):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., score + 0.01, f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Cluster size comparison\n",
    "cluster_sizes_kmeans = rfm['Cluster'].value_counts().sort_index()\n",
    "cluster_sizes_agglo = rfm['Agglo_Cluster'].value_counts().sort_index()\n",
    "x = np.arange(len(cluster_sizes_kmeans))\n",
    "width = 0.35\n",
    "\n",
    "ax5.bar(x - width/2, cluster_sizes_kmeans.values, width, label='K-Means', color='skyblue')\n",
    "ax5.bar(x + width/2, cluster_sizes_agglo.values, width, label='Agglomerative', color='lightcoral')\n",
    "\n",
    "ax5.set_title('Cluster Size Comparison')\n",
    "ax5.set_xlabel('Cluster')\n",
    "ax5.set_ylabel('Number of Customers')\n",
    "ax5.set_xticks(x)\n",
    "\n",
    "# Use the actual cluster indices for labels\n",
    "ax5.set_xticklabels([f'Cluster {i}' for i in cluster_sizes_kmeans.index])\n",
    "ax5.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fc6bf1-fbab-4227-b202-b03f8a718841",
   "metadata": {},
   "source": [
    "## Collaborative Filtering Recommendation System\n",
    "\n",
    "To further explore on additional unsupervised machine learning algo, I implemented a collaborative filtering recommendation system using both user-based and item-based approaches on the Online Retail dataset. The customer-product matrix contained 4312 customers and 4444 products, with a sparsity of 98.5%. To handle the wide range of purchase quantities, values were log-scaled using np.log1p.\n",
    "\n",
    "The system computes cosine similarities for both customers and products. For item-based CF, a shrinkage factor is applied to adjust for co-purchase counts, reducing noise from items with few interactions.\n",
    "\n",
    "User-based CF generates predictions by weighting purchases of the 20 most similar customers, centering by each customer's mean to account for different purchasing scales. Item-based CF predicts scores by summing contributions from previously purchased items weighted by similarity.\n",
    "\n",
    "The dataset was split into training and test sets, masking 20% of interactions for testing. RMSE evaluation on the test set gave the following results:\n",
    "\n",
    "| Method     | RMSE    |\n",
    "|------------|---------|\n",
    "| User-CF    | 2.0217  |\n",
    "| Item-CF    | 19.8311 |\n",
    "\n",
    "This indicate that user-based CF provides more accurate predictions in this setting, likely because item-based CF struggles with the high sparsity and limited co-purchase information. Overall, collaborative filtering demonstrates the ability to generate relevant recommendations and can be used to enhance customer experience and suggest new products.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09b4c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class CFRecommendationSystem:\n",
    "    \n",
    "    def __init__(self, customer_product_matrix: pd.DataFrame, shrinkage_lambda=10):\n",
    "        self.customer_product_matrix = np.log1p(customer_product_matrix)\n",
    "        self.customer_similarity = None\n",
    "        self.product_similarity = None\n",
    "        self.shrinkage_lambda = shrinkage_lambda\n",
    "        \n",
    "    def compute_similarities(self):\n",
    "        self.customer_similarity = cosine_similarity(self.customer_product_matrix)\n",
    "        \n",
    "        raw_item_similarity = cosine_similarity(self.customer_product_matrix.T)\n",
    "        \n",
    "        bin_matrix = (self.customer_product_matrix > 0).astype(float).T.values\n",
    "        co_counts = bin_matrix @ bin_matrix.T\n",
    "        \n",
    "        self.product_similarity = raw_item_similarity * co_counts / (co_counts + self.shrinkage_lambda)\n",
    "        np.fill_diagonal(self.product_similarity, 1.0)\n",
    "    \n",
    "    def predict_user_cf(self, customer_id: int) -> np.ndarray:\n",
    "        if customer_id not in self.customer_product_matrix.index:\n",
    "            return np.zeros(self.customer_product_matrix.shape[1])\n",
    "        \n",
    "        customer_idx = self.customer_product_matrix.index.get_loc(customer_id)\n",
    "        similarities = self.customer_similarity[customer_idx]\n",
    "        \n",
    "        similar_customers_idx = similarities.argsort()[-21:][::-1][1:]\n",
    "        weights = similarities[similar_customers_idx]\n",
    "        \n",
    "        if weights.sum() == 0:\n",
    "            return np.zeros(self.customer_product_matrix.shape[1])\n",
    "        \n",
    "        similar_customers_purchases = self.customer_product_matrix.iloc[similar_customers_idx]\n",
    "        \n",
    "        mean_user = self.customer_product_matrix.iloc[customer_idx].mean()\n",
    "        mean_neighbors = similar_customers_purchases.mean(axis=1)\n",
    "        centered_neighbors = similar_customers_purchases.sub(mean_neighbors, axis=0)\n",
    "        \n",
    "        pred = mean_user + (centered_neighbors.T * weights).sum(axis=1) / weights.sum()\n",
    "        return np.clip(pred.values, 0, None)\n",
    "    \n",
    "    def predict_item_cf(self, customer_id: int) -> np.ndarray:\n",
    "        if customer_id not in self.customer_product_matrix.index:\n",
    "            return np.zeros(self.customer_product_matrix.shape[1])\n",
    "        \n",
    "        customer_purchases = self.customer_product_matrix.loc[customer_id]\n",
    "        purchased_items = customer_purchases[customer_purchases > 0].index\n",
    "        \n",
    "        if len(purchased_items) == 0:\n",
    "            return np.zeros(len(customer_purchases))\n",
    "        \n",
    "        scores = pd.Series(0.0, index=self.customer_product_matrix.columns)\n",
    "        for item in purchased_items:\n",
    "            item_idx = self.customer_product_matrix.columns.get_loc(item)\n",
    "            scores += self.product_similarity[item_idx] * customer_purchases[item]\n",
    "        \n",
    "        return np.clip(scores.values, 0, None)\n",
    "    \n",
    "    def evaluate_rmse(self, test_matrix: pd.DataFrame, method='user') -> float:\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        \n",
    "        test_matrix_scaled = np.log1p(test_matrix)\n",
    "        \n",
    "        for customer_id in self.customer_product_matrix.index:\n",
    "            if method == 'user':\n",
    "                pred = self.predict_user_cf(customer_id)\n",
    "            else:\n",
    "                pred = self.predict_item_cf(customer_id)\n",
    "            \n",
    "            actual = test_matrix_scaled.loc[customer_id].values\n",
    "            mask = actual > 0\n",
    "            if mask.sum() > 0:\n",
    "                predictions.extend(pred[mask])\n",
    "                actuals.extend(actual[mask])\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "        return rmse\n",
    "\n",
    "\n",
    "\n",
    "customer_product_matrix = df_clean.pivot_table(\n",
    "    index='Customer ID',\n",
    "    columns='Description',\n",
    "    values='Quantity',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n",
    "matrix = customer_product_matrix.values.copy()\n",
    "test_mask = np.random.rand(*matrix.shape) < 0.2\n",
    "\n",
    "train_matrix = matrix.copy()\n",
    "train_matrix[test_mask] = 0\n",
    "\n",
    "test_matrix = matrix.copy()\n",
    "test_matrix[~test_mask] = 0\n",
    "\n",
    "train_df = pd.DataFrame(train_matrix, index=customer_product_matrix.index, columns=customer_product_matrix.columns)\n",
    "test_df = pd.DataFrame(test_matrix, index=customer_product_matrix.index, columns=customer_product_matrix.columns)\n",
    "\n",
    "# Train CF Recommendataion system\n",
    "cf_system = CFRecommendationSystem(train_df)\n",
    "cf_system.compute_similarities()\n",
    "\n",
    "user_rmse = cf_system.evaluate_rmse(test_df, method='user')\n",
    "item_rmse = cf_system.evaluate_rmse(test_df, method='item')\n",
    "\n",
    "print(f\"User-CF RMSE: {user_rmse:.4f}\")\n",
    "print(f\"Item-CF RMSE: {item_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c96520b",
   "metadata": {},
   "source": [
    "This example shows recommendations for a sample customer (ID: 12346). The already purchased items include mostly doormats and a few umbrellas and parasols. \n",
    "\n",
    "The top 10 user-based CF recommendations suggest items that similar customers have purchased, introducing some new products like hanging heart t-light holders and ceramic trinket boxes. \n",
    "\n",
    "The top 10 item-based CF recommendations focus on products similar to those already bought, reinforcing familiar items like union flag doormats and hearts doormats, along with related home accessories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample customer\n",
    "sample_customer_id = train_df.index[0]\n",
    "top_n = 10\n",
    "\n",
    "# --- Already purchased items ---\n",
    "purchased_mask = train_df.loc[sample_customer_id] > 0\n",
    "purchased_items = train_df.columns[purchased_mask]\n",
    "\n",
    "# --- User-CF recommendations ---\n",
    "user_pred = cf_system.predict_user_cf(sample_customer_id)\n",
    "user_pred_masked = np.where(purchased_mask, -np.inf, user_pred)\n",
    "top_user_idx = np.argsort(user_pred_masked)[-top_n:][::-1]\n",
    "user_recommended_items = train_df.columns[top_user_idx]\n",
    "\n",
    "# --- Item-CF recommendations ---\n",
    "item_pred = cf_system.predict_item_cf(sample_customer_id)\n",
    "item_pred_masked = np.where(purchased_mask, -np.inf, item_pred)\n",
    "top_item_idx = np.argsort(item_pred_masked)[-top_n:][::-1]\n",
    "item_recommended_items = train_df.columns[top_item_idx]\n",
    "\n",
    "# --- Display results ---\n",
    "print(f\"Customer ID: {sample_customer_id}\\n\")\n",
    "\n",
    "print(\"Already purchased items:\")\n",
    "for i, item in enumerate(purchased_items, 1):\n",
    "    print(f\"{i}. {item}\")\n",
    "\n",
    "print(\"\\nTop User-CF recommended items:\")\n",
    "for i, item in enumerate(user_recommended_items, 1):\n",
    "    print(f\"{i}. {item}\")\n",
    "\n",
    "print(\"\\nTop Item-CF recommended items:\")\n",
    "for i, item in enumerate(item_recommended_items, 1):\n",
    "    print(f\"{i}. {item}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6df7392",
   "metadata": {},
   "source": [
    "This heatmap visualizes the pairwise cosine similarity between a sample of 20 customers. Higher values (warmer colors) indicate more similar purchase behavior, helping to identify clusters of customers with similar preferences for user-based recommendations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d274a-fe8b-422c-9f63-8e3ccd8ab713",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = list(range(20))\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cf_system.customer_similarity[np.ix_(sample_idx, sample_idx)],\n",
    "            cmap=\"coolwarm\", annot=True, fmt=\".2f\")\n",
    "plt.title(\"Customer Similarity Heatmap (Sample 20 Customers)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc958f9",
   "metadata": {},
   "source": [
    "This heatmap shows the similarity between the top 20 most purchased products. Warmer colors indicate higher similarity, reflecting items that are frequently bought together and guiding item-based recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec54c64f-d7fd-4016-9d1f-b396a4fbe736",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_products = customer_product_matrix.sum().sort_values(ascending=False).index[:20]\n",
    "product_idx = [customer_product_matrix.columns.get_loc(p) for p in top_products]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cf_system.product_similarity[np.ix_(product_idx, product_idx)],\n",
    "            xticklabels=top_products, yticklabels=top_products,\n",
    "            cmap=\"coolwarm\", annot=True, fmt=\".2f\")\n",
    "plt.title(\"Product Similarity Heatmap (Top 20 Products)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7bdc5",
   "metadata": {},
   "source": [
    "### NMF Latent Factors & Sales Forecast\n",
    "\n",
    "To extend it further to try if can use NMF for this online sales dataset, I refer to one paper from here https://www.jstage.jst.go.jp/article/ipsjjip/27/0/27_752/_pdf, and applied NMF to the Online Retail dataset to extract latent factors for products and customers. First, the purchase quantities were aggregated into a tensor of shape `(products × customers × months)` and log-scaled using `np.log1p`. Averaging over the time dimension produced a `(products × customers)` matrix for NMF.\n",
    "\n",
    "A hyperparameter sweep was conducted over latent dimensions from 5 to 30. As the number of components increased, the reconstruction mean squared error (MSE) decreased, indicating better approximation of the original data. For example, with 5 latent factors the MSE was 0.000952, while with 30 factors it dropped to 0.000708. This helps select the optimal number of latent factors for capturing patterns in the data.\n",
    "\n",
    "The resulting factor matrices consist of product latent factors (`U`) of shape `(num_products × K)` and customer latent factors (`V`) of shape `(K × num_customers)`. Each row of `U` represents a product’s strength across latent patterns such as home décor or kitchenware, while each column of `V` represents a customer’s preference for each latent factor.\n",
    "\n",
    "Forecasting can be performed linearly over time for each element of the latent matrices, producing predicted matrices `U_forecast` and `V_forecast`. The final reconstruction `X_pred = U_forecast @ V_forecast` gives a `(num_products × num_customers)` matrix that estimates the likelihood of a customer purchasing a product in the future. This approach allows for capturing temporal trends and hidden patterns in customer-product interactions.\n",
    "\n",
    "This is quite interesting to implement NMF to make the sales forcasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be15cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_clean['InvoiceDate'] = pd.to_datetime(df_clean['InvoiceDate'])\n",
    "df_clean['Month'] = df_clean['InvoiceDate'].dt.to_period('M')\n",
    "\n",
    "customers = df_clean['Customer ID'].unique()\n",
    "products = df_clean['StockCode'].unique()\n",
    "months = df_clean['Month'].sort_values().unique()\n",
    "T = len(months)\n",
    "\n",
    "customer_to_idx = {c: i for i, c in enumerate(customers)}\n",
    "product_to_idx = {p: i for i, p in enumerate(products)}\n",
    "month_to_idx = {m: i for i, m in enumerate(months)}\n",
    "\n",
    "df_clean['CustomerIdx'] = df_clean['Customer ID'].map(customer_to_idx)\n",
    "df_clean['ProductIdx'] = df_clean['StockCode'].map(product_to_idx)\n",
    "df_clean['MonthIdx'] = df_clean['Month'].map(month_to_idx)\n",
    "\n",
    "# --- Build tensor Z (product x customer x month) ---\n",
    "Z = np.zeros((len(products), len(customers), T), dtype=np.float32)\n",
    "indices = df_clean[['ProductIdx', 'CustomerIdx', 'MonthIdx']].values\n",
    "quantities = df_clean['Quantity'].values\n",
    "Z[indices[:, 0], indices[:, 1], indices[:, 2]] = quantities\n",
    "\n",
    "Z = np.log1p(Z)\n",
    "\n",
    "# --- Average over time dimension ---\n",
    "XAve = Z.mean(axis=2)  # product x customer matrix\n",
    "\n",
    "# --- hyperparameter sweep for latent factors ---\n",
    "component_list = [5, 10, 15, 20, 25, 30, 35, 40]\n",
    "results = []\n",
    "\n",
    "for K in component_list:\n",
    "    try:\n",
    "        nmf_model = NMF(\n",
    "            n_components=K,\n",
    "            init='nndsvda',\n",
    "            random_state=42,\n",
    "            max_iter=500\n",
    "        )\n",
    "        U = nmf_model.fit_transform(XAve)\n",
    "        V = nmf_model.components_\n",
    "        X_reconstructed = U @ V\n",
    "        mse = mean_squared_error(XAve, X_reconstructed)\n",
    "        results.append({'n_components': K, 'MSE': mse})\n",
    "        print(f\"K={K} -> MSE: {mse:.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"K={K} failed: {e}\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\hyperparameter Sweep Results:\")\n",
    "print(df_results)\n",
    "\n",
    "# --- Plot MSE vs n_components ---\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(df_results['n_components'], df_results['MSE'], marker='o')\n",
    "plt.xlabel('Latent Dimensions (n_components)')\n",
    "plt.ylabel('Reconstruction MSE')\n",
    "plt.title('NMF Parameter Sweep')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a783c6c7",
   "metadata": {},
   "source": [
    "## Choosing NMF Components and Recommendation Threshold\n",
    "\n",
    "For NMF, the MSE steadily decreases as `n_components` increases, but the rate of improvement slows down after around 20–25 components. To balance reconstruction accuracy and model simplicity, I have chosen n_components = 20.\n",
    "\n",
    "The recommendation threshold is currently set at 0.8 to reduce false positives, meaning that only the most confident predictions are recommended. \n",
    "\n",
    "## Future Improvement\n",
    "\n",
    "In future improvements, the threshold could be determined using the 95th percentile of predicted scores, which would select the top 5% of predictions and may lead to better overall recommendation performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7019faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "df_clean['InvoiceDate'] = pd.to_datetime(df_clean['InvoiceDate'])\n",
    "df_clean['Month'] = df_clean['InvoiceDate'].dt.to_period('M')\n",
    "\n",
    "customers = df_clean['Customer ID'].unique()\n",
    "products = df_clean['StockCode'].unique()\n",
    "months = df_clean['Month'].sort_values().unique()\n",
    "T = len(months)\n",
    "\n",
    "customer_to_idx = {c: i for i, c in enumerate(customers)}\n",
    "product_to_idx = {p: i for i, p in enumerate(products)}\n",
    "month_to_idx = {m: i for i, m in enumerate(months)}\n",
    "\n",
    "df_clean['CustomerIdx'] = df_clean['Customer ID'].map(customer_to_idx)\n",
    "df_clean['ProductIdx'] = df_clean['StockCode'].map(product_to_idx)\n",
    "df_clean['MonthIdx'] = df_clean['Month'].map(month_to_idx)\n",
    "\n",
    "# --- Create 3D tensor: products x customers x months ---\n",
    "Z = np.zeros((len(products), len(customers), T), dtype=np.float32)\n",
    "indices = df_clean[['ProductIdx','CustomerIdx','MonthIdx']].values\n",
    "quantities = df_clean['Quantity'].values\n",
    "Z[indices[:,0], indices[:,1], indices[:,2]] = quantities\n",
    "\n",
    "Z = np.log1p(Z) \n",
    "\n",
    "XAve = Z.mean(axis=2)  # average over time to get a static product-customer matrix\n",
    "K = 20\n",
    "nmf_model = NMF(n_components=K, init='nndsvda', random_state=42, max_iter=500)\n",
    "Uinit = nmf_model.fit_transform(XAve)  # customer latent factors\n",
    "Vinit = nmf_model.components_          # product latent factors\n",
    "\n",
    "def linear_forecast(matrix_seq):\n",
    "    T, dim1, dim2 = matrix_seq.shape\n",
    "    x = np.arange(T)\n",
    "    forecast = np.zeros((dim1, dim2), dtype=np.float32)\n",
    "    \n",
    "    for i in range(dim1):\n",
    "        for j in range(dim2):\n",
    "            y = matrix_seq[:, i, j]\n",
    "            mask = y != 0\n",
    "            if mask.sum() > 1:\n",
    "                coeff = np.polyfit(x[mask], y[mask], 1) \n",
    "                forecast[i, j] = coeff[0]*T + coeff[1] \n",
    "            elif mask.sum() == 1:\n",
    "                forecast[i, j] = y[mask][0]\n",
    "    return np.maximum(forecast, 0)\n",
    "\n",
    "U_forecast = linear_forecast(np.broadcast_to(Uinit, (T, *Uinit.shape)))\n",
    "V_forecast = linear_forecast(np.broadcast_to(Vinit, (T, *Vinit.shape)))\n",
    "\n",
    "X_pred = U_forecast @ V_forecast\n",
    "threshold = 0.8\n",
    "X_pred_binary = (X_pred > threshold).astype(np.int8)\n",
    "\n",
    "prod_idx, cust_idx = np.nonzero(X_pred_binary)\n",
    "df_pred = pd.DataFrame({\n",
    "    'CustomerID': customers[cust_idx],\n",
    "    'ProductID': products[prod_idx],\n",
    "    'Predicted': 1\n",
    "})\n",
    "\n",
    "print(df_pred.shape)\n",
    "print(df_pred.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d238d4f",
   "metadata": {},
   "source": [
    "## Heatmap of XAve\n",
    "\n",
    "This heatmap displays a subset of the XAve matrix (average log-quantity) for 100 products and 100 customers. Darker shades correspond to higher average purchase quantities, emphasizing the most frequently bought products and the most active customers in XAve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c598e523-40ad-4ee0-a830-b6d41e461db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sample_customers = 100\n",
    "sample_products = 100\n",
    "XAve_sample = XAve[:sample_products, :sample_customers]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(XAve_sample, cmap='YlGnBu', cbar_kws={'label': 'Average Log-Quantity'})\n",
    "plt.xlabel('Customers')\n",
    "plt.ylabel('Products')\n",
    "plt.title('Heatmap of Average Product-Customer Matrix (XAve)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f56e06",
   "metadata": {},
   "source": [
    "## Heatmap of Predicted Purchases\n",
    "\n",
    "This heatmap visualizes the predicted customer-product purchases from df_pred. Cells with darker blue indicate a higher likelihood that the customer will purchase the product, highlighting the model's top recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458cca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_heat = df_pred.pivot(index='CustomerID', columns='ProductID', values='Predicted').fillna(0)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.heatmap(df_heat, cmap='Blues', cbar_kws={'label':'Predicted Purchase'})\n",
    "plt.title(\"Predicted Customer-Product Purchases\")\n",
    "plt.xlabel(\"Product\")\n",
    "plt.ylabel(\"Customer\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa877da8",
   "metadata": {},
   "source": [
    "# Shows all predicted products for Customer 14911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c2b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_14911_pred = df_pred[df_pred['CustomerID'] == 14911]\n",
    "print(customer_14911_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddefcd9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This project was my attempt to apply unsupervised machine learning techniques to the UCI Online Retail Dataset. It was quite challenging at first, but I learned a lot along the way. I started by cleaning the messy dataset, removing about 25% of bad data, which was more than I expected. After that, I explored customer purchasing patterns through visualizations and used RFM analysis to understand customer behavior in terms of recency, frequency, and monetary value. To handle the high dimensionality of the data, I applied PCA for dimensionality reduction. I then tried both K-Means and Agglomerative clustering to segment customers into three groups. Additionally, I built collaborative filtering recommendation systems, though the data was extremely sparse, with 98.5% missing values.\n",
    "\n",
    "The key findings from this project were insightful. I identified three distinct customer segments: high-value loyal customers, moderate occasional buyers, and low-value at-risk customers. Between the clustering methods, Agglomerative clustering performed slightly better than K-Means, achieving a silhouette score of 0.41 compared to 0.39. In the recommendation system, user-based collaborative filtering performed much better than item-based, with an RMSE of 2.02 versus 19.83. Overall, the clustering produced clear and interpretable groups, which was helpful for understanding customer behavior. However, the recommendation task was challenging due to the high sparsity of the data, as most customers purchased only a few items.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This project taught me that real-world data is often messy and difficult to work with. While I was able to successfully identify customer segments that make business sense, I realized there is still a lot more to learn about handling sparse data and selecting the most suitable algorithms. I learned that data preprocessing is far more important than I initially thought, and that different clustering algorithms can provide different insights into the same dataset. Sparsity is a major issue in recommendation systems, and visualization plays a critical role in understanding how algorithms operate and what patterns they capture.\n",
    "\n",
    "## Next Step\n",
    "For the next steps, I explored NMF for forecasting, which I found particularly interesting. I implemented NMF on a 3D tensor of products, customers, and months, then averaged over time to extract latent factors. Inspired by a paper I read, this approach worked reasonably well, with MSE decreasing from 0.000952 using 5 factors to 0.000708 with 30 factors. I believe there is still much more to explore in this area, including optimizing the latent factors and experimenting with different tensor factorization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be7646b",
   "metadata": {},
   "source": [
    "# reference\n",
    "https://www.jstage.jst.go.jp/article/ipsjjip/27/0/27_752/_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad5b704",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
